<h2>Text normalization</h2>
<html>
 <head></head>
 <body>
  <p>In this topic, we are going to learn more about text normalization, one of the steps of text preprocessing. Let's imagine that we have some text and want to count all instances of the verb <em>play</em>. Sounds easy, right? What about word forms like "<em>played"</em>, "<em>plays"</em>, or "<em>playing"</em>? They are all forms of one single verb. We can count them manually if our text is short, but with big data, it is just not possible. This is where <strong>text normalization</strong> (or <strong>word normalization) </strong>steps in. The main idea is to reduce different forms of one word to a single form. With this algorithm all forms like "<em>plays"</em>, "<em>playing"</em>, or "<em>played"</em> will be changed to "<em>play"</em>.</p> 
  <p>There are two approaches to text normalization: <strong>stemming</strong> and <strong>lemmatization</strong>. Both are widely used in information retrieval tasks, search engines, topic modeling, and other NLP applications. In the upcoming sections, we will discuss the differences between the approaches, as well as their implementations in the NLTK library.</p> 
  <p>Note that before stemming or lemmatization, it is better to tokenize your text and get rid of digits and punctuation marks. Otherwise, most algorithms will recognize "<em>play!"</em> not as "<em>play"</em>, but rather as an unknown word, so it will not be processed correctly.</p> 
  <h5>Stemming</h5> 
  <p><strong>Stem </strong>is the most important part of the word, and other word parts (called <em>affixes) </em>are added to it. For instance, if we take the stem <em>play</em> and add the affix <em>-ed</em> to it, we get the past form of the verb<em> play</em>.</p> 
  <p>In the process of <strong>stemming</strong>, we remove all affixes to get a stem. Note that the result may not represent a real word and it is okay! For example, for the word "<em>ladies",</em> we may have "<em>ladi"</em>.</p> 
  <p>Let's see how to carry it out using NLTK. It has different algorithms for stemming and we will learn how to use them. First, we need to import the library:</p> 
  <pre><code class="language-python">import nltk</code></pre> 
  <p>For the English language, we normally use the Porter stemmer and the Lancaster stemmer. You can find these and some other stemming algorithms in the <code class="language-python">nltk.stem</code> module.</p> 
  <p>The <strong>Porter stemmer</strong> is the earliest and the most popular algorithm for this task. To use it, we need to import the <code class="language-python">PorterStemmer</code> class from the <code class="language-python">nltk.stem</code> module and then create an object of this class. It is used only for English. After that we call the <code class="language-python">stem()</code> method and put the word in brackets:</p> 
  <pre><code class="language-python">from nltk.stem import PorterStemmer


porter = PorterStemmer()
porter.stem('played')   # play
porter.stem('playing')  # play</code></pre> 
  <p>The <strong>Snowball stemmer</strong> can be seen as an improvement over the original Porter stemmer as it gives slightly better results. The <code class="language-python">SnowballStemmer</code> class in NLTK also supports 13 non-English languages such as Spanish, French, Russian, German, Swedish, and others. To use this algorithm, we need to create a new instance of the class and specify the language.</p> 
  <pre><code class="language-python">from nltk.stem import SnowballStemmer


snowball = SnowballStemmer('english')
snowball.stem('playing')  # play
snowball.stem('played')   # play</code></pre> 
  <p>As we said earlier, the Snowball stemmer works better than Porter. Let's compare the examples below:</p> 
  <pre><code class="language-python">snowball.stem('generously')   # generous
porter.stem('generously')     # gener

snowball.stem('dangerously')  # danger
porter.stem('dangerously')    # danger</code></pre> 
  <p>The Porter stemmer would remove not only the affix "<em>-ly" </em>but also "<em>-ous" </em>from the input, as it would do for the word "<em>dangerously"</em>. In this case, it is unnecessary and incorrect. The Snowball stemmer provides a better result for the word "<em>generously"</em>.</p> 
  <p>NLTK also has the implementation of the Lancaster or Paice-Husk stemming algorithm. To use the <strong>Lancaster stemmer</strong>, we need to do the same as before, but now we need to import the <code class="language-python">LancasterStemmer</code> class from the <code class="language-python">nltk.stem</code> package:</p> 
  <pre><code class="language-python">from nltk.stem import LancasterStemmer


lancaster = LancasterStemmer()  
lancaster.stem('played')       # play
lancaster.stem('playing')      # play
lancaster.stem('generously' )  # gen
lancaster.stem('dangerously')  # dang</code></pre> 
  <p>All stemmers are quite similar, but the original Porter stemmer and Snowball stemmer provide better results, while the Lancaster stemmer works faster. So, if you are working with really big text data and need to process it in a short time, use Lancaster Stemmer. If you need more accurate results — choose the Snowball or Porter stemmers.</p> 
  <p>You can learn more about the non-English stemmers available in NLTK <a target="_blank" href="https://www.nltk.org/api/nltk.stem.html#module-nltk.stem" rel="noopener noreferrer nofollow">on nltk.org</a>.</p> 
  <h5>Lemmatization</h5> 
  <p>Now let's talk about <strong>lemmatization</strong>. Even though it may seem similar to stemming at first sight, there is a difference in how these algorithms work. Stemmers just remove affixes while <strong>lemmatizers</strong> are like people — they analyze the word, its context, its part of speech, and then give the answer. The result is always a real word in its dictionary form called a <strong>lemma</strong>. In general, lemmatizers rely on dictionaries (or corpora) when looking for lemmas.</p> 
  <p>To use lemmatizer from the NLTK library, you need to make sure that you have access to WordNet — you can do it by typing <code class="language-python">nltk.download('wordnet')</code>.</p> 
  <p><strong>WordNet</strong> is a large lexical database of the English language, which is used for lemmas in the NLTK lemmatizer. There is only one algorithm for lemmatization in NLTK.</p> 
  <p>We need to import the <code class="language-python">WordNetLemmatizer</code> class from the <code class="language-python">nltk.stem</code> module and create an instance of the class. We use the method <code class="language-python">lemmatize()</code> that takes a word we want to lemmatize as an argument.</p> 
  <pre><code class="language-python">from nltk.stem import WordNetLemmatizer


lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('playing')  # playing</code></pre> 
  <p>As you can see, the word remained unchanged after lemmatization. As far as we know, lemmatizers need to know the context or the part of speech of the word. The default part of speech here is a noun and, as a noun, the word <em>'playing'</em> is its own lemma. In the small chart below you can find the most common part-of-speech tags in WordNet:</p> 
  <table align="center" border="1" cellpadding="1" cellspacing="1" style="height: 181px; width: 254px;"> 
   <tbody> 
    <tr> 
     <td style="text-align: center;"><strong>Part of speech</strong></td> 
     <td style="text-align: center;"><strong>Tag</strong></td> 
    </tr> 
    <tr> 
     <td style="text-align: center;">Noun</td> 
     <td style="text-align: center;">n</td> 
    </tr> 
    <tr> 
     <td style="text-align: center;">Verb</td> 
     <td style="text-align: center;">v</td> 
    </tr> 
    <tr> 
     <td style="text-align: center;">Adjective</td> 
     <td style="text-align: center;">a</td> 
    </tr> 
   </tbody> 
  </table> 
  <p>So, we just need to assign the tag that corresponds to the part of speech for our word.</p> 
  <pre><code class="language-python">lemmatizer.lemmatize('playing', pos='v')  # play
lemmatizer.lemmatize('plays')             # play</code></pre> 
  <p>When we lemmatize a text we cannot manually tag all words, so you need to define a function that will assign a part-of-speech tag to each word.</p> 
  <p></p>
  <div class="alert alert-primary">
   Note that part-of-speech tags must be the same as in the WordNet! If the tags you got in the result of POS-tagging do not correspond to the ones in the WordNet, you will need to convert them.
  </div>
  <p></p> 
  <h5>Stemming vs. Lemmatization</h5> 
  <p>What should you choose? The answer to this question mainly depends on the task and the language you are dealing with. There is no universal stemmer or lemmatizer for all languages — each language is unique and has specific rules. So you need to use different algorithms with different languages.</p> 
  <p>For some languages, both stemming and lemmatization give good results, but for others, it is better to opt for lemmatization. For instance, languages like Russian, Latin, Finnish, or Turkish have grammatical <strong>cases</strong>, meaning that words have different affixes depending on their role in a sentence. Here is an example from Latin: "<em>rēx respondit"</em> can be translated as "<em>the king replied"</em>, and <em>rēgis fīlia</em> — as "<em>the daughter of the king"</em>. Both these words, "<em>rēx"</em> and "<em>rēgis"</em>, are forms of the noun "<em>rēx"</em>, which stands for "<em>king".</em> Cutting off the affixes will give us two different forms, so it is better to apply lemmatization here. Of course, it is possible to use stemming for such languages, but the list of rules what affixes in which cases should be removed is going to be pretty long and complex.</p> 
  <p>Also, if you need to get valid words after text normalization, go for lemmatization. Sometimes, different forms of one word look completely different, and we just cannot write rules for them. For instance, in English, there are irregular verbs (<em>be</em> — <em>am</em>, <em>is</em>, <em>are</em>), plural forms of nouns (<em>goose</em> — <em>geese</em>, <em>mouse</em> — <em>mice</em>), and comparative and superlative adjectives (<em>bad</em> — <em>worse</em> — <em>the worst</em>). Lemmatizers will detect such cases and give the correct word as a result, while stemmers will not. You can see the example below:</p> 
  <pre><code class="language-python">#stemming
snowball.stem('worst')                   # worst
snowball.stem('bought')                  # bought
snowball.stem('mice')                    # mice

#lemmatization
lemmatizer.lemmatize('worst', pos='a')   # bad
lemmatizer.lemmatize('bought', pos='v')  # buy
lemmatizer.lemmatize('mice')             # mouse</code></pre> 
  <p>Finally, do not forget about the resources. Lemmatizers usually scan a big dictionary or rely on corpora to find lemmas. It can take a lot of time. If you need to normalize text faster, stemming is the right choice.</p> 
  <p>Let's sum up the main points of using stemming and lemmatization.</p> 
  <table align="center" border="1" cellpadding="1" cellspacing="1" style="width: 800px;"> 
   <tbody> 
    <tr> 
     <td> </td> 
     <td style="text-align: center;"><strong>Stemming</strong></td> 
     <td style="text-align: center;"><strong>Lemmatization</strong></td> 
    </tr> 
    <tr> 
     <td><strong>Pros</strong></td> 
     <td> 
      <ul> 
       <li>works fast (good for big data)</li> 
       <li>gives good results for some languages (English)</li> 
       <li>does not require much memory </li> 
      </ul> </td> 
     <td> 
      <ul> 
       <li>gives a valid word as a result</li> 
       <li>recognizes cases of suppletion</li> 
      </ul> </td> 
    </tr> 
    <tr> 
     <td><strong>Cons</strong></td> 
     <td> 
      <ul> 
       <li>gives as result a stem that may not be a real word</li> 
      </ul> </td> 
     <td> 
      <ul> 
       <li>takes longer to process</li> 
      </ul> </td> 
    </tr> 
   </tbody> 
  </table> 
  <h5>Other implementations</h5> 
  <p>NLTK is not the only library that has implementations of text normalization algorithms. Below is a list of libraries where you can find implementations of text normalization for English:</p> 
  <ul> 
   <li><a target="_blank" href="https://pypi.org/project/hunspell/" rel="noopener noreferrer nofollow">Hunspell</a> (stemming);</li> 
   <li><a target="_blank" href="https://radimrehurek.com/gensim/parsing/porter.html" rel="noopener noreferrer nofollow">Gensim</a> (stemming);</li> 
   <li><a target="_blank" href="https://spacy.io/api/lemmatizer" rel="noopener noreferrer nofollow">SpaCy</a> (lemmatization);</li> 
   <li><a target="_blank" href="https://textblob.readthedocs.io/en/dev/quickstart.html" rel="noopener noreferrer nofollow">TextBlob</a> (lemmatization);</li> 
   <li><a target="_blank" href="https://github.com/clips/pattern/wiki/pattern-en" rel="noopener noreferrer nofollow">Pattern</a> (lemmatization).</li> 
  </ul> 
  <h5>Summary</h5> 
  <p>In this topic, we have learned about text preprocessing and the role of text normalization in it, the difference between two main approaches (stemming and lemmatization), and how to implement some algorithms using NLTK. Let's recap:</p> 
  <ul> 
   <li><strong>Text normalization</strong> is an important step of text preprocessing. It reduces various word forms to one single form.</li> 
   <li>There are two approaches to text normalization: <strong>stemming</strong> removes affixes according to some rules and keeps the <strong>stem</strong>, while <strong>lemmatization</strong> analyzes the word and returns its <strong>lemma</strong> with the help of a dictionary.</li> 
   <li>Both stemming and lemmatization have their advantages and disadvantages. Stemming works faster than lemmatization but the latter is usually more precise and always returns a real word.</li> 
  </ul>
 </body>
</html>
